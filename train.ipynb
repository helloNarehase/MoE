{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers, ops\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = ops.arange(n_dest)[:, None]\n",
    "    j = ops.arange(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(m, dtype)\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = ops.concatenate(\n",
    "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return ops.tile(mask, mult)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(0, maxlen, 1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # The ffn can be either a standard feedforward network or a switch\n",
    "        # layer with a Mixture of Experts.\n",
    "        self.ffn = ffn\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "\n",
    "def load_balanced_loss(router_probs, expert_mask):\n",
    "    # router_probs [tokens_per_batch, num_experts] is the probability assigned for\n",
    "    # each expert per token. expert_mask [tokens_per_batch, num_experts] contains\n",
    "    # the expert with the highest router probability in one−hot format.\n",
    "\n",
    "    num_experts = ops.shape(expert_mask)[-1]\n",
    "    # Get the fraction of tokens routed to each expert.\n",
    "    # density is a vector of length num experts that sums to 1.\n",
    "    density = ops.mean(expert_mask, axis=0)\n",
    "    # Get fraction of probability mass assigned to each expert from the router\n",
    "    # across all tokens. density_proxy is a vector of length num experts that sums to 1.\n",
    "    density_proxy = ops.mean(router_probs, axis=0)\n",
    "    # Want both vectors to have uniform allocation (1/num experts) across all\n",
    "    # num_expert elements. The two vectors will be pushed towards uniform allocation\n",
    "    # when the dot product is minimized.\n",
    "    loss = ops.mean(density_proxy * density) * ops.cast((num_experts**2), \"float32\")\n",
    "    return loss\n",
    "\n",
    "def create_feedforward_network(ff_dim, embed_dim, name=None):\n",
    "    return keras.Sequential(\n",
    "        [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim)], name=name\n",
    "    )\n",
    "\n",
    "class Router(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_capacity):\n",
    "        self.num_experts = num_experts\n",
    "        self.route = layers.Dense(units=num_experts)\n",
    "        self.expert_capacity = expert_capacity\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs shape: [tokens_per_batch, embed_dim]\n",
    "        # router_logits shape: [tokens_per_batch, num_experts]\n",
    "        router_logits = self.route(inputs)\n",
    "\n",
    "        if training:\n",
    "            # Add noise for exploration across experts.\n",
    "            router_logits += keras.random.uniform(\n",
    "                shape=router_logits.shape, minval=0.9, maxval=1.1\n",
    "            )\n",
    "        # Probabilities for each token of what expert it should be sent to.\n",
    "        router_probs = keras.activations.softmax(router_logits, axis=-1)\n",
    "        # Get the top−1 expert for each token. expert_gate is the top−1 probability\n",
    "        # from the router for each token. expert_index is what expert each token\n",
    "        # is going to be routed to.\n",
    "        expert_gate, expert_index = ops.top_k(router_probs, k=1)\n",
    "        # expert_mask shape: [tokens_per_batch, num_experts]\n",
    "        expert_mask = ops.one_hot(expert_index, self.num_experts)\n",
    "        # Compute load balancing loss.\n",
    "        aux_loss = load_balanced_loss(router_probs, expert_mask)\n",
    "        self.add_loss(aux_loss)\n",
    "        # Experts have a fixed capacity, ensure we do not exceed it. Construct\n",
    "        # the batch indices, to each expert, with position in expert make sure that\n",
    "        # not more that expert capacity examples can be routed to each expert.\n",
    "        position_in_expert = ops.cast(\n",
    "            ops.cumsum(expert_mask, axis=0) * expert_mask, \"int32\"\n",
    "        )\n",
    "        # Keep only tokens that fit within expert capacity.\n",
    "        expert_mask *= ops.cast(\n",
    "            ops.less(ops.cast(position_in_expert, \"int32\"), self.expert_capacity),\n",
    "            \"float32\",\n",
    "        )\n",
    "        expert_mask_flat = ops.sum(expert_mask, axis=-1)\n",
    "        # Mask out the experts that have overflowed the expert capacity.\n",
    "        expert_gate *= expert_mask_flat\n",
    "        # Combine expert outputs and scaling with router probability.\n",
    "        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n",
    "        combined_tensor = ops.expand_dims(\n",
    "            expert_gate\n",
    "            * expert_mask_flat\n",
    "            * ops.squeeze(ops.one_hot(expert_index, self.num_experts), 1),\n",
    "            -1,\n",
    "        ) * ops.squeeze(ops.one_hot(position_in_expert, self.expert_capacity), 1)\n",
    "        # Create binary dispatch_tensor [tokens_per_batch, num_experts, expert_capacity]\n",
    "        # that is 1 if the token gets routed to the corresponding expert.\n",
    "        dispatch_tensor = ops.cast(combined_tensor, \"float32\")\n",
    "\n",
    "        return dispatch_tensor, combined_tensor\n",
    "    \n",
    "class Switch(layers.Layer):\n",
    "    def __init__(\n",
    "        self, num_experts, embed_dim, ff_dim, num_tokens_per_batch, capacity_factor=1, maxlen = 128\n",
    "    ):\n",
    "        self.maxlen = maxlen\n",
    "        self.num_experts = num_experts\n",
    "        self.embed_dim = embed_dim\n",
    "        self.experts = [\n",
    "            create_feedforward_network(ff_dim, embed_dim) for _ in range(num_experts)\n",
    "        ]\n",
    "\n",
    "        self.expert_capacity = num_tokens_per_batch // self.num_experts\n",
    "        self.router = Router(self.num_experts, self.expert_capacity)\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = ops.shape(inputs)[0]\n",
    "        num_tokens_per_example = ops.shape(inputs)[1]\n",
    "\n",
    "        # inputs shape: [num_tokens_per_batch, embed_dim]\n",
    "        inputs = ops.reshape(inputs, [self.maxlen, self.embed_dim])\n",
    "        # dispatch_tensor shape: [expert_capacity, num_experts, tokens_per_batch]\n",
    "        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n",
    "        dispatch_tensor, combine_tensor = self.router(inputs)\n",
    "        # expert_inputs shape: [num_experts, expert_capacity, embed_dim]\n",
    "        expert_inputs = ops.einsum(\"ab,acd->cdb\", inputs, dispatch_tensor)\n",
    "        expert_inputs = ops.reshape(\n",
    "            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n",
    "        )\n",
    "        # Dispatch to experts\n",
    "        expert_input_list = ops.unstack(expert_inputs, axis=0)\n",
    "        expert_output_list = [\n",
    "            self.experts[idx](expert_input)\n",
    "            for idx, expert_input in enumerate(expert_input_list)\n",
    "        ]\n",
    "        # expert_outputs shape: [expert_capacity, num_experts, embed_dim]\n",
    "        expert_outputs = ops.stack(expert_output_list, axis=1)\n",
    "        # expert_outputs_combined shape: [tokens_per_batch, embed_dim]\n",
    "        expert_outputs_combined = ops.einsum(\n",
    "            \"abc,xba->xc\", expert_outputs, combine_tensor\n",
    "        )\n",
    "        # output shape: [batch_size, num_tokens_per_example, embed_dim]\n",
    "        outputs = ops.reshape(\n",
    "            expert_outputs_combined,\n",
    "            [batch_size, num_tokens_per_example, self.embed_dim],\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class param:\n",
    "    maxlen = 256\n",
    "    num_experts = 8\n",
    "    ffn_dim = 768\n",
    "    layers = 8\n",
    "    num_heads = 12\n",
    "    vocaps = 200000\n",
    "    dropout_rate = 0.2\n",
    "    embed_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_131\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_131\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ input_layer_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ token_and_position_embedding_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">51,265,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_8             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_9             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_10            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_11            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_12            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_14            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_15            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,416,060</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_273 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200000</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">51,400,000</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ input_layer_65 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ token_and_position_embedding_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │ \u001b[38;5;34m51,265,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_8             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_9             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_10            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_11            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_12            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_14            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ transformer_block_15            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │  \u001b[38;5;34m3,416,060\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_273 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200000\u001b[0m)      │ \u001b[38;5;34m51,400,000\u001b[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">129,994,016</span> (495.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m129,994,016\u001b[0m (495.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">129,994,016</span> (495.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m129,994,016\u001b[0m (495.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers, ops\n",
    "\n",
    "params = param()\n",
    "\n",
    "def create_classifier():\n",
    "    inputs = layers.Input(shape=(None,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(\n",
    "        params.maxlen, params.vocaps, params.embed_dim\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    for i in range(params.layers): \n",
    "        switch = Switch(params.num_experts, params.embed_dim, params.ffn_dim, params.maxlen)\n",
    "        transformer_block = TransformerBlock(params.embed_dim // params.num_heads, params.num_heads, switch)\n",
    "        x = transformer_block(x)\n",
    "    outputs = layers.Dense(params.vocaps)(x)\n",
    "    classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return classifier\n",
    "\n",
    "model = create_classifier()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128, 200000), dtype=float32, numpy=\n",
       "array([[[ 0.06726842,  0.06890401,  0.07971887, ...,  0.00160627,\n",
       "          0.07661902, -0.07421705],\n",
       "        [ 0.02440177,  0.1568586 , -0.014684  , ...,  0.01056107,\n",
       "          0.130892  ,  0.00140067],\n",
       "        [ 0.02978635,  0.02690938, -0.06342617, ..., -0.05049652,\n",
       "          0.00212195,  0.03306109],\n",
       "        ...,\n",
       "        [ 0.01830697,  0.06606097,  0.01168641, ...,  0.04482744,\n",
       "          0.11705779, -0.06354382],\n",
       "        [ 0.08233418,  0.04788302,  0.07058999, ..., -0.02281084,\n",
       "          0.07388151, -0.07273614],\n",
       "        [ 0.0650604 ,  0.05699984, -0.02073703, ..., -0.02300578,\n",
       "          0.15101725, -0.11141483]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras-Tensor]",
   "language": "python",
   "name": "conda-env-keras-Tensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
